{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Deep Learning Models\n",
        "\n",
        "This notebook demonstrates how to build and train deep learning models for network anomaly detection.\n",
        "\n",
        "## Overview\n",
        "- Convert data to sequences for time-series models\n",
        "- Build and train Keras models (LSTM, CNN, Autoencoder)\n",
        "- Visualize training history and model performance\n",
        "- Compare different deep learning architectures\n",
        "\n",
        "## Models\n",
        "We'll explore LSTM, 1D-CNN, and Autoencoder architectures for anomaly detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning imports\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers, models, callbacks\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "    TENSORFLOW_AVAILABLE = True\n",
        "    print(\"✅ TensorFlow imported successfully!\")\n",
        "except ImportError as e:\n",
        "    TENSORFLOW_AVAILABLE = False\n",
        "    print(f\"❌ TensorFlow not available: {e}\")\n",
        "    print(\"Install with: pip install tensorflow\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "if TENSORFLOW_AVAILABLE:\n",
        "    tf.random.set_seed(RANDOM_SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data\n",
        "\n",
        "Let's load the processed data and prepare it for deep learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "data_path = Path(\"../data/processed/processed.csv\")\n",
        "\n",
        "if data_path.exists():\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"✅ Data loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"❌ Processed data not found. Please run preprocessing first.\")\n",
        "    print(\"Run: python ../src/preprocess.py --input ../data/raw/sample.csv --output ../data/processed/processed.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = df.drop(columns=['label']).values\n",
        "y = df['label'].values\n",
        "\n",
        "# Convert to binary classification (0: BENIGN, 1: Anomaly)\n",
        "y_binary = (y != 0).astype(int)\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Labels shape: {y_binary.shape}\")\n",
        "print(f\"Binary label distribution: {np.bincount(y_binary)}\")\n",
        "print(f\"Label distribution (percentages): {np.bincount(y_binary) / len(y_binary) * 100}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Sequence Preparation\n",
        "\n",
        "For time-series models like LSTM and CNN, we need to convert our data into sequences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sequences(X, y, sequence_length=10):\n",
        "    \"\"\"\n",
        "    Convert features into sequences for time-series models.\n",
        "    \n",
        "    Args:\n",
        "        X: Input features\n",
        "        y: Target labels\n",
        "        sequence_length: Length of each sequence\n",
        "        \n",
        "    Returns:\n",
        "        sequences: Array of sequences\n",
        "        labels: Array of corresponding labels\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    \n",
        "    for i in range(len(X) - sequence_length + 1):\n",
        "        # Take a window of features\n",
        "        seq = X[i:i + sequence_length]\n",
        "        # Use the label of the last element in the sequence\n",
        "        label = y[i + sequence_length - 1]\n",
        "        \n",
        "        sequences.append(seq)\n",
        "        labels.append(label)\n",
        "    \n",
        "    return np.array(sequences), np.array(labels)\n",
        "\n",
        "# Create sequences\n",
        "sequence_length = 10\n",
        "X_seq, y_seq = create_sequences(X, y_binary, sequence_length)\n",
        "\n",
        "print(f\"Original data shape: {X.shape}\")\n",
        "print(f\"Sequence data shape: {X_seq.shape}\")\n",
        "print(f\"Sequence labels shape: {y_seq.shape}\")\n",
        "print(f\"Sequence length: {sequence_length}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train/test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n",
        ")\n",
        "\n",
        "# Further split training data for validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(f\"Training labels distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test labels distribution: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build LSTM Model\n",
        "\n",
        "Let's build and train an LSTM model for sequence classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Build LSTM model\n",
        "    def build_lstm_model(input_shape, num_classes=1):\n",
        "        model = models.Sequential([\n",
        "            # First LSTM layer with return sequences for stacked architecture\n",
        "            layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "            layers.Dropout(0.2),\n",
        "            \n",
        "            # Second LSTM layer\n",
        "            layers.LSTM(32, return_sequences=False),\n",
        "            layers.Dropout(0.2),\n",
        "            \n",
        "            # Dense layers for classification\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            layers.Dropout(0.1),\n",
        "            \n",
        "            # Output layer for binary classification\n",
        "            layers.Dense(num_classes, activation='sigmoid')\n",
        "        ])\n",
        "        \n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    # Build LSTM model\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    lstm_model = build_lstm_model(input_shape)\n",
        "    \n",
        "    print(\"LSTM Model Architecture:\")\n",
        "    lstm_model.summary()\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot build LSTM model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Train LSTM model\n",
        "    print(\"Training LSTM model...\")\n",
        "    \n",
        "    # Define callbacks\n",
        "    callbacks_list = [\n",
        "        callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=5,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=3,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        )\n",
        "    ]\n",
        "    \n",
        "    # Train model\n",
        "    history_lstm = lstm_model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=10,  # Small number for demo\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"✅ LSTM training completed!\")\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot train LSTM model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build CNN Model\n",
        "\n",
        "Let's build and train a 1D CNN model for sequence classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Build CNN model\n",
        "    def build_cnn_model(input_shape, num_classes=1):\n",
        "        model = models.Sequential([\n",
        "            # First Conv1D layer\n",
        "            layers.Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(pool_size=2),\n",
        "            layers.Dropout(0.2),\n",
        "            \n",
        "            # Second Conv1D layer\n",
        "            layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
        "            layers.BatchNormalization(),\n",
        "            layers.MaxPooling1D(pool_size=2),\n",
        "            layers.Dropout(0.2),\n",
        "            \n",
        "            # Global max pooling to reduce dimensions\n",
        "            layers.GlobalMaxPooling1D(),\n",
        "            \n",
        "            # Dense layers for classification\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dropout(0.3),\n",
        "            layers.Dense(16, activation='relu'),\n",
        "            \n",
        "            # Output layer for binary classification\n",
        "            layers.Dense(num_classes, activation='sigmoid')\n",
        "        ])\n",
        "        \n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "        \n",
        "        return model\n",
        "    \n",
        "    # Build CNN model\n",
        "    cnn_model = build_cnn_model(input_shape)\n",
        "    \n",
        "    print(\"CNN Model Architecture:\")\n",
        "    cnn_model.summary()\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot build CNN model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Train CNN model\n",
        "    print(\"Training CNN model...\")\n",
        "    \n",
        "    # Train model\n",
        "    history_cnn = cnn_model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=10,  # Small number for demo\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"✅ CNN training completed!\")\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot train CNN model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Build Autoencoder Model\n",
        "\n",
        "Let's build an autoencoder for unsupervised anomaly detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Build Autoencoder model\n",
        "    def build_autoencoder_model(input_dim):\n",
        "        # Encoder\n",
        "        encoder_input = layers.Input(shape=(input_dim,), name='encoder_input')\n",
        "        encoder = layers.Dense(64, activation='relu')(encoder_input)\n",
        "        encoder = layers.Dropout(0.2)(encoder)\n",
        "        encoder = layers.Dense(32, activation='relu')(encoder)\n",
        "        encoder = layers.Dropout(0.2)(encoder)\n",
        "        encoder = layers.Dense(16, activation='relu', name='encoder_output')(encoder)\n",
        "        \n",
        "        # Decoder\n",
        "        decoder = layers.Dense(32, activation='relu')(encoder)\n",
        "        decoder = layers.Dropout(0.2)(decoder)\n",
        "        decoder = layers.Dense(64, activation='relu')(decoder)\n",
        "        decoder = layers.Dropout(0.2)(decoder)\n",
        "        decoder = layers.Dense(input_dim, activation='linear', name='decoder_output')(decoder)\n",
        "        \n",
        "        # Create autoencoder model\n",
        "        autoencoder = models.Model(encoder_input, decoder, name='autoencoder')\n",
        "        \n",
        "        # Compile model\n",
        "        autoencoder.compile(\n",
        "            optimizer='adam',\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "        \n",
        "        return autoencoder\n",
        "    \n",
        "    # Build Autoencoder model\n",
        "    input_dim = X_train.shape[2]  # Number of features\n",
        "    autoencoder_model = build_autoencoder_model(input_dim)\n",
        "    \n",
        "    print(\"Autoencoder Model Architecture:\")\n",
        "    autoencoder_model.summary()\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot build Autoencoder model\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Train Autoencoder model\n",
        "    print(\"Training Autoencoder model...\")\n",
        "    \n",
        "    # For autoencoder, use only normal data for training\n",
        "    normal_mask = y_train == 0\n",
        "    X_train_normal = X_train[normal_mask]\n",
        "    \n",
        "    print(f\"Training autoencoder on {len(X_train_normal)} normal samples\")\n",
        "    \n",
        "    # Train model\n",
        "    history_autoencoder = autoencoder_model.fit(\n",
        "        X_train_normal, X_train_normal,  # Autoencoder: input = target\n",
        "        validation_data=(X_val, X_val),\n",
        "        epochs=10,  # Small number for demo\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks_list,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    print(\"✅ Autoencoder training completed!\")\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot train Autoencoder model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Plot Training History\n",
        "\n",
        "Let's visualize the training progress for all models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Plot training history for LSTM and CNN\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # LSTM Loss\n",
        "    axes[0, 0].plot(history_lstm.history['loss'], label='Training Loss')\n",
        "    axes[0, 0].plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
        "    axes[0, 0].set_title('LSTM - Loss')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # LSTM Accuracy\n",
        "    axes[0, 1].plot(history_lstm.history['accuracy'], label='Training Accuracy')\n",
        "    axes[0, 1].plot(history_lstm.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[0, 1].set_title('LSTM - Accuracy')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Accuracy')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # CNN Loss\n",
        "    axes[1, 0].plot(history_cnn.history['loss'], label='Training Loss')\n",
        "    axes[1, 0].plot(history_cnn.history['val_loss'], label='Validation Loss')\n",
        "    axes[1, 0].set_title('CNN - Loss')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('Loss')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # CNN Accuracy\n",
        "    axes[1, 1].plot(history_cnn.history['accuracy'], label='Training Accuracy')\n",
        "    axes[1, 1].plot(history_cnn.history['val_accuracy'], label='Validation Accuracy')\n",
        "    axes[1, 1].set_title('CNN - Accuracy')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot plot training history\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Plot Autoencoder training history\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Autoencoder Loss\n",
        "    axes[0].plot(history_autoencoder.history['loss'], label='Training Loss')\n",
        "    axes[0].plot(history_autoencoder.history['val_loss'], label='Validation Loss')\n",
        "    axes[0].set_title('Autoencoder - Loss (MSE)')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Autoencoder MAE\n",
        "    axes[1].plot(history_autoencoder.history['mae'], label='Training MAE')\n",
        "    axes[1].plot(history_autoencoder.history['val_mae'], label='Validation MAE')\n",
        "    axes[1].set_title('Autoencoder - Mean Absolute Error')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('MAE')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot plot autoencoder training history\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Evaluation\n",
        "\n",
        "Let's evaluate the performance of our trained models on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Evaluate LSTM model\n",
        "    print(\"Evaluating LSTM model...\")\n",
        "    lstm_pred = lstm_model.predict(X_test, verbose=0)\n",
        "    lstm_pred_binary = (lstm_pred > 0.5).astype(int).flatten()\n",
        "    \n",
        "    print(\"LSTM Classification Report:\")\n",
        "    print(classification_report(y_test, lstm_pred_binary))\n",
        "    \n",
        "    # Evaluate CNN model\n",
        "    print(\"\\nEvaluating CNN model...\")\n",
        "    cnn_pred = cnn_model.predict(X_test, verbose=0)\n",
        "    cnn_pred_binary = (cnn_pred > 0.5).astype(int).flatten()\n",
        "    \n",
        "    print(\"CNN Classification Report:\")\n",
        "    print(classification_report(y_test, cnn_pred_binary))\n",
        "    \n",
        "    # Evaluate Autoencoder model\n",
        "    print(\"\\nEvaluating Autoencoder model...\")\n",
        "    autoencoder_pred = autoencoder_model.predict(X_test, verbose=0)\n",
        "    \n",
        "    # Calculate reconstruction error\n",
        "    mse = np.mean(np.square(X_test - autoencoder_pred), axis=1)\n",
        "    \n",
        "    # Use reconstruction error as anomaly score\n",
        "    threshold = np.percentile(mse, 95)  # 95th percentile as threshold\n",
        "    autoencoder_pred_binary = (mse > threshold).astype(int)\n",
        "    \n",
        "    print(\"Autoencoder Classification Report:\")\n",
        "    print(classification_report(y_test, autoencoder_pred_binary))\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot evaluate models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Plot ROC curves for comparison\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
        "    \n",
        "    # LSTM ROC\n",
        "    fpr_lstm, tpr_lstm, _ = roc_curve(y_test, lstm_pred)\n",
        "    roc_auc_lstm = roc_auc_score(y_test, lstm_pred)\n",
        "    ax.plot(fpr_lstm, tpr_lstm, label=f'LSTM (AUC = {roc_auc_lstm:.4f})')\n",
        "    \n",
        "    # CNN ROC\n",
        "    fpr_cnn, tpr_cnn, _ = roc_curve(y_test, cnn_pred)\n",
        "    roc_auc_cnn = roc_auc_score(y_test, cnn_pred)\n",
        "    ax.plot(fpr_cnn, tpr_cnn, label=f'CNN (AUC = {roc_auc_cnn:.4f})')\n",
        "    \n",
        "    # Autoencoder ROC\n",
        "    fpr_ae, tpr_ae, _ = roc_curve(y_test, mse)\n",
        "    roc_auc_ae = roc_auc_score(y_test, mse)\n",
        "    ax.plot(fpr_ae, tpr_ae, label=f'Autoencoder (AUC = {roc_auc_ae:.4f})')\n",
        "    \n",
        "    # Random classifier\n",
        "    ax.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "    \n",
        "    ax.set_xlim([0.0, 1.0])\n",
        "    ax.set_ylim([0.0, 1.05])\n",
        "    ax.set_xlabel('False Positive Rate')\n",
        "    ax.set_ylabel('True Positive Rate')\n",
        "    ax.set_title('ROC Curves Comparison')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot plot ROC curves\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Models\n",
        "\n",
        "Let's save the trained models for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TENSORFLOW_AVAILABLE:\n",
        "    # Create models directory if it doesn't exist\n",
        "    models_dir = Path(\"../models\")\n",
        "    models_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Save models\n",
        "    lstm_model.save(\"../models/dl_lstm.h5\")\n",
        "    print(\"✅ LSTM model saved to models/dl_lstm.h5\")\n",
        "    \n",
        "    cnn_model.save(\"../models/dl_cnn.h5\")\n",
        "    print(\"✅ CNN model saved to models/dl_cnn.h5\")\n",
        "    \n",
        "    autoencoder_model.save(\"../models/dl_autoencoder.h5\")\n",
        "    print(\"✅ Autoencoder model saved to models/dl_autoencoder.h5\")\n",
        "    \n",
        "    # Save model metadata\n",
        "    metadata = {\n",
        "        'sequence_length': sequence_length,\n",
        "        'input_shape': input_shape,\n",
        "        'models_trained': ['lstm', 'cnn', 'autoencoder'],\n",
        "        'training_epochs': 10,\n",
        "        'random_seed': RANDOM_SEED\n",
        "    }\n",
        "    \n",
        "    import joblib\n",
        "    joblib.dump(metadata, \"../models/dl_models_metadata.joblib\")\n",
        "    print(\"✅ Model metadata saved to models/dl_models_metadata.joblib\")\n",
        "else:\n",
        "    print(\"TensorFlow not available - cannot save models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary and Next Steps\n",
        "\n",
        "### Key Findings:\n",
        "1. **Model Performance**: [To be filled based on actual results]\n",
        "2. **Best Architecture**: [To be filled based on actual results]\n",
        "3. **Training Convergence**: [To be filled based on actual results]\n",
        "\n",
        "### Model Comparison:\n",
        "- **LSTM**: Good for capturing temporal dependencies in sequences\n",
        "- **CNN**: Effective for local pattern recognition in sequences\n",
        "- **Autoencoder**: Unsupervised approach for anomaly detection\n",
        "\n",
        "### Next Steps:\n",
        "1. **Hyperparameter Tuning**: Optimize model architectures and training parameters\n",
        "2. **Ensemble Methods**: Combine multiple models for improved performance\n",
        "3. **Advanced Architectures**: Try Transformer, GRU, or hybrid models\n",
        "4. **Data Augmentation**: Generate more training data for better generalization\n",
        "5. **Transfer Learning**: Use pre-trained models for feature extraction\n",
        "6. **Model Compression**: Optimize models for deployment\n",
        "7. **Real-time Inference**: Implement streaming prediction capabilities\n",
        "8. **Production Deployment**: Deploy models using the FastAPI service\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
