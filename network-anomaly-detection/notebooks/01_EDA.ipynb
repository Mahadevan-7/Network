{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 01 - Exploratory Data Analysis (EDA)\n",
        "\n",
        "This notebook performs comprehensive exploratory data analysis on the processed network anomaly detection dataset.\n",
        "\n",
        "## Overview\n",
        "- Load and examine processed data\n",
        "- Analyze class distribution and data quality\n",
        "- Visualize feature distributions and relationships\n",
        "- Check for missing values and data integrity\n",
        "\n",
        "## Dataset\n",
        "The processed dataset contains network flow features that have been cleaned, encoded, and scaled for machine learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Data\n",
        "\n",
        "First, let's load the processed dataset and examine its basic structure.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed data\n",
        "data_path = Path(\"../data/processed/processed.csv\")\n",
        "\n",
        "if data_path.exists():\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"✅ Data loaded successfully!\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "else:\n",
        "    print(\"❌ Processed data not found. Please run preprocessing first.\")\n",
        "    print(\"Run: python ../src/preprocess.py --input ../data/raw/sample.csv --output ../data/processed/processed.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic dataset information\n",
        "print(\"Dataset Info:\")\n",
        "print(f\"Rows: {df.shape[0]}\")\n",
        "print(f\"Columns: {df.shape[1]}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "print(\"\\nData types:\")\n",
        "print(df.dtypes.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Class Distribution Analysis\n",
        "\n",
        "Let's examine the distribution of classes in our dataset to understand the balance between normal and anomalous traffic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if label column exists\n",
        "if 'label' in df.columns:\n",
        "    print(\"Label column found!\")\n",
        "    print(f\"Label distribution:\")\n",
        "    print(df['label'].value_counts())\n",
        "    print(f\"\\nLabel distribution (percentages):\")\n",
        "    print(df['label'].value_counts(normalize=True) * 100)\n",
        "else:\n",
        "    print(\"❌ Label column not found in dataset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution\n",
        "if 'label' in df.columns:\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "    \n",
        "    # Count plot\n",
        "    df['label'].value_counts().plot(kind='bar', ax=ax1, color=['skyblue', 'salmon'])\n",
        "    ax1.set_title('Class Distribution (Counts)')\n",
        "    ax1.set_xlabel('Class')\n",
        "    ax1.set_ylabel('Count')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Pie chart\n",
        "    df['label'].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%', startangle=90)\n",
        "    ax2.set_title('Class Distribution (Percentages)')\n",
        "    ax2.set_ylabel('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Analysis\n",
        "\n",
        "Let's examine the features in our dataset to understand their distributions and characteristics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "if 'label' in df.columns:\n",
        "    features = df.drop(columns=['label'])\n",
        "    target = df['label']\n",
        "    \n",
        "    print(f\"Number of features: {features.shape[1]}\")\n",
        "    print(f\"Feature names: {list(features.columns)}\")\n",
        "else:\n",
        "    features = df\n",
        "    target = None\n",
        "    print(\"No target column found, analyzing all columns as features\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic statistics for numeric features\n",
        "print(\"Basic Statistics for Numeric Features:\")\n",
        "print(features.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select a subset of features for visualization (to avoid overcrowding)\n",
        "n_features_to_plot = min(10, features.shape[1])\n",
        "selected_features = features.columns[:n_features_to_plot]\n",
        "\n",
        "print(f\"Plotting distributions for first {n_features_to_plot} features:\")\n",
        "print(selected_features.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot histograms for selected features\n",
        "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, feature in enumerate(selected_features):\n",
        "    if i < len(axes):\n",
        "        axes[i].hist(features[feature], bins=30, alpha=0.7, edgecolor='black')\n",
        "        axes[i].set_title(f'{feature}', fontsize=10)\n",
        "        axes[i].set_xlabel('Value')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "# Hide unused subplots\n",
        "for i in range(len(selected_features), len(axes)):\n",
        "    axes[i].set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Correlation Analysis\n",
        "\n",
        "Let's examine the correlations between features to understand relationships in our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrix for selected features\n",
        "correlation_matrix = features[selected_features].corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, \n",
        "            annot=True, \n",
        "            cmap='coolwarm', \n",
        "            center=0,\n",
        "            square=True,\n",
        "            fmt='.2f',\n",
        "            cbar_kws={'shrink': 0.8})\n",
        "plt.title('Feature Correlation Matrix')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Missing Values and Data Quality\n",
        "\n",
        "Let's check for missing values and assess data quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (missing_values / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing_Count': missing_values.values,\n",
        "    'Missing_Percentage': missing_percentage.values\n",
        "})\n",
        "\n",
        "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
        "\n",
        "if len(missing_df) > 0:\n",
        "    print(\"❌ Missing values found:\")\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"✅ No missing values found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for infinite values\n",
        "infinite_values = np.isinf(features.select_dtypes(include=[np.number])).sum()\n",
        "infinite_df = pd.DataFrame({\n",
        "    'Column': infinite_values.index,\n",
        "    'Infinite_Count': infinite_values.values\n",
        "})\n",
        "\n",
        "infinite_df = infinite_df[infinite_df['Infinite_Count'] > 0]\n",
        "\n",
        "if len(infinite_df) > 0:\n",
        "    print(\"❌ Infinite values found:\")\n",
        "    print(infinite_df)\n",
        "else:\n",
        "    print(\"✅ No infinite values found in numeric features!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Duplicate rows: {duplicate_count}\")\n",
        "\n",
        "if duplicate_count > 0:\n",
        "    print(f\"Percentage of duplicates: {(duplicate_count / len(df)) * 100:.2f}%\")\n",
        "else:\n",
        "    print(\"✅ No duplicate rows found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Distribution by Class\n",
        "\n",
        "Let's examine how features are distributed across different classes to identify patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot feature distributions by class (if target exists)\n",
        "if target is not None and len(target.unique()) > 1:\n",
        "    # Select a few key features for visualization\n",
        "    key_features = selected_features[:4] if len(selected_features) >= 4 else selected_features\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for i, feature in enumerate(key_features):\n",
        "        if i < len(axes):\n",
        "            for class_label in target.unique():\n",
        "                class_data = features[target == class_label][feature]\n",
        "                axes[i].hist(class_data, alpha=0.6, label=f'Class {class_label}', bins=20)\n",
        "            \n",
        "            axes[i].set_title(f'{feature} by Class')\n",
        "            axes[i].set_xlabel('Value')\n",
        "            axes[i].set_ylabel('Frequency')\n",
        "            axes[i].legend()\n",
        "            axes[i].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for i in range(len(key_features), len(axes)):\n",
        "        axes[i].set_visible(False)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Cannot plot by class - target variable not available or has only one class\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary and Next Steps\n",
        "\n",
        "### Key Findings:\n",
        "1. **Dataset Size**: [To be filled based on actual data]\n",
        "2. **Class Distribution**: [To be filled based on actual data]\n",
        "3. **Feature Quality**: [To be filled based on actual data]\n",
        "4. **Missing Values**: [To be filled based on actual data]\n",
        "\n",
        "### Next Steps:\n",
        "1. **Feature Engineering**: Consider creating new features based on domain knowledge\n",
        "2. **Feature Selection**: Identify the most important features for classification\n",
        "3. **Model Training**: Proceed to train baseline ML models\n",
        "4. **Deep Learning**: Explore deep learning approaches for sequence data\n",
        "5. **Hyperparameter Tuning**: Optimize model parameters for better performance\n",
        "6. **Model Evaluation**: Implement comprehensive evaluation metrics\n",
        "7. **Deployment**: Prepare models for production deployment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Distribution by Class\n",
        "\n",
        "Let's examine how features are distributed across different classes to identify patterns.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
